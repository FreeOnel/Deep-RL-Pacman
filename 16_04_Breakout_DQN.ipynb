{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Breakout_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOa9hxl/RPidNHdHjBkPg2E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FreeOnel/Deep-RL-Pacman/blob/main/16_04_Breakout_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCKTaoMtnV67"
      },
      "source": [
        "\"\"\" REFERENCES\n",
        "General structure from - https://keras.io/examples/rl/deep_q_network_breakout/\n",
        "Prioritised replay from - https://github.com/cocolico14/N-step-Dueling-DDQN-PER-Pacman\n",
        "Preprocessing wrapper from - https://github.com/openai/gym/blob/master/gym/wrappers/atari_preprocessing.py\n",
        "Deepmind rainbow dqn paper - https://arxiv.org/pdf/1710.02298.pdf\n",
        "Atari environment - https://gym.openai.com/envs/Breakout-v0/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEiiKEEBsCmX",
        "outputId": "f58302e8-25cb-48e7-af58-732bd3662930"
      },
      "source": [
        "!pip install wandb\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install -U colabgymrender"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.26)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.2.0)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n",
            "Requirement already up-to-date: colabgymrender in /usr/local/lib/python3.7/dist-packages (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (2.1)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: ipython in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: moviepy in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (0.2.3.5)\n",
            "Requirement already satisfied, skipping upgrade: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay->colabgymrender) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python->colabgymrender) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (54.2.0)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (5.0.5)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->colabgymrender) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->colabgymrender) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->colabgymrender) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->colabgymrender) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKjwH8-8zPcR"
      },
      "source": [
        "# Import necessary modules\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import heapq\n",
        "from collections import deque\n",
        "import random\n",
        "from itertools import count\n",
        "#from colabgymrender.recorder import Recorder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urEMxtZI0C8f"
      },
      "source": [
        "# Set parameters\n",
        "\n",
        "\n",
        "gamma = 0.99                                          # Discount factor \n",
        "epsilon = 1.0                                         # Epsilon greedy parameter\n",
        "epsilon_min = 0.1                                     # Minimum epsilon greedy parameter\n",
        "epsilon_max = 1.0                                     # Maximum epsilon greedy parameter\n",
        "epsilon_interval = (epsilon_max - epsilon_min)        # Rate at which to reduce chance of random action being taken\n",
        "epsilon_decay_steps = 300000                          # Number of steps over which to decay epsilon from max to min\n",
        "batch_size = 32                                       # Size of batch taken from replay buffer\n",
        "max_steps_per_episode = 10000                         # Set max to prevent episodes running too long\n",
        "replay_steps = 4                                      # Perform replay after every 4th step\n",
        "target_update = 10000                                  # Update target model with weights from main model every 10000 steps\n",
        "max_buffer = 50000                                    # Max buffer length to save memory space\n",
        "alpha = 0.6                                           # Replay prioritisation parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLuL1YnyxfEL"
      },
      "source": [
        "from gym.wrappers import AtariPreprocessing\n",
        "\n",
        "# Instantiate environment\n",
        "\n",
        "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
        "action_space = 4\n",
        "\n",
        "# Built in preprocessor wrapper converts observations (grayscale, downsize, crop) and performs frame stacking\n",
        "# Print np.shape(env.reset()) before and after to see changes\n",
        "env = AtariPreprocessing(env)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woNBTnU6mmHq"
      },
      "source": [
        "# Create VANILLA Deep Q-Network\n",
        "\n",
        "def create_model():\n",
        "    # Network architecture as specified in Deepmind's *ADD YEAR* paper\n",
        "    inputs = layers.Input(shape=(84, 84, 1,))\n",
        "\n",
        "    # 3 convolutional layers with relu activation\n",
        "    layer_1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
        "    layer_2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer_1)\n",
        "    layer_3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer_2)\n",
        "\n",
        "    flat_layer_3 = layers.Flatten()(layer_3)\n",
        "\n",
        "    # 2 fully connected layers, with no activation on the final layer\n",
        "    layer_4 = layers.Dense(512, activation=\"relu\")(flat_layer_3)\n",
        "    layer_5 = layers.Dense(action_space, activation=\"linear\")(layer_4)\n",
        "\n",
        "    # Input is a state representation, output is action values\n",
        "    return keras.Model(inputs=inputs, outputs=layer_5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wxv4Eey2Dp2"
      },
      "source": [
        "# Create DUELLING Deep Q-Network\n",
        "\"\"\"\n",
        "def create_model():\n",
        "    # Network architecture as specified in Deepmind's *ADD YEAR* paper\n",
        "    inputs = layers.Input(shape=(84, 84, 1,))\n",
        "\n",
        "    # 3 convolutional layers with relu activation\n",
        "    layer_1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
        "    layer_2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer_1)\n",
        "    layer_3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer_2)\n",
        "\n",
        "    flat_layer_3 = layers.Flatten()(layer_3)\n",
        "\n",
        "    # Duelling network\n",
        "    value_layer = layers.Dense(1, activation='linear')(flat_layer_3)\n",
        "    advantage_layer = layers.Dense(action_space, activation='linear')(flat_layer_3)\n",
        "\n",
        "    Q = value_layer + tf.subtract(advantage_layer, tf.reduce_mean(advantage_layer, axis=1, keepdims=True))\n",
        "\n",
        "    # Input is a state representation, output is action values\n",
        "    return keras.Model(inputs=inputs, outputs=Q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rGBA0RI_FzC"
      },
      "source": [
        "# Create main and target q-networks\n",
        "\n",
        "model = create_model()\n",
        "target_model = create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPva6V66zSE9"
      },
      "source": [
        "# Use Adam optimizer to optimize efficiency of training\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.00025)\n",
        "\n",
        "# Define Huber loss function\n",
        "loss_function = keras.losses.Huber()\n",
        "\n",
        "# Compile main and target models with optimizer and loss function\n",
        "model.compile(optimizer, loss_function)\n",
        "target_model.compile(optimizer, loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSzHIorVzrl2"
      },
      "source": [
        "# Define epsilon greedy step\n",
        "\n",
        "def epsilon_greedy(action, step):\n",
        "  epsilon = max(epsilon_min, epsilon_max - epsilon_interval * step/epsilon_decay_steps) # Decay epsilon with more steps\n",
        "  if step % 1000 == 0:\n",
        "    print(\"epsilon = \", epsilon, \"at step \", step)\n",
        "  if np.random.rand() < epsilon:\n",
        "    return np.random.choice(action_space)\n",
        "  else:\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8NoY3xG7vDD"
      },
      "source": [
        " # Calculate TD error for use in prioritised replay\n",
        " \n",
        " def calculate_td_error(transition):\n",
        "        state, action, reward, next_state, Terminal = transition\n",
        "        if not Terminal:\n",
        "            next_state = tf.expand_dims(next_state, 0)\n",
        "            all_actions = model(next_state, training=False)\n",
        "            max_action = np.argmax(all_actions[0])\n",
        "            target_q = (reward + gamma * target_model(next_state, training=False)[0][max_action])\n",
        "        else:\n",
        "            target_q = reward\n",
        "\n",
        "        q = model(state, training=False)[0][action]\n",
        "        \n",
        "        td = tf.cast(q - target_q, dtype=tf.float32)\n",
        "        td = td.numpy()\n",
        "\n",
        "        return td"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_BGoCJt_dvX"
      },
      "source": [
        "# Define replay\n",
        "\n",
        "def replay(batch_size):\n",
        "\n",
        "  # Semi Stochastic Prioritization\n",
        "  prioritization = int(batch_size*alpha)\n",
        "  batch_prioritized = heapq.nlargest(prioritization, buffer)\n",
        "  batch_uniform = random.sample(buffer, batch_size-prioritization)\n",
        "  batch = batch_prioritized + batch_uniform\n",
        "\n",
        "  batch = [e for (_, _, e) in batch]\n",
        "  states = []\n",
        "  targets = []\n",
        "\n",
        "  for state, action, reward, next_state, Terminal in batch:\n",
        "    if not Terminal:\n",
        "        next_s = np.expand_dims(next_state, axis=0)\n",
        "        # Double DQN\n",
        "        max_action = np.argmax(model(next_s, training=False)[0])\n",
        "        target_q = (reward + gamma * target_model(next_s, training=False)[0][max_action])\n",
        "    else:\n",
        "        target_q = reward\n",
        "\n",
        "    #current_s = np.expand_dims(state, axis=0)\n",
        "    q = model(state, training=False)\n",
        "    q = tf.make_ndarray(tf.make_tensor_proto(q))\n",
        "    q[0][int(action)] = target_q\n",
        "    states.append(state)\n",
        "    targets.append(q.reshape(action_space))\n",
        "\n",
        "  states = np.reshape(np.array(states), (32, 84, 84, 1))\n",
        "  model.fit(states, np.array(targets), batch_size=batch_size, epochs=1, verbose=0)\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "FcSHIuIL-xxv",
        "outputId": "a8f36cf3-28e9-438a-90ca-0ad328ef4691"
      },
      "source": [
        "# Record training with wand api\n",
        "wb = True\n",
        "if wb:\n",
        "  wandb.init(project='Breakout', entity='pacman_dqn')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.26<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">vibrant-galaxy-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/pacman_dqn/Breakout\" target=\"_blank\">https://wandb.ai/pacman_dqn/Breakout</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/pacman_dqn/Breakout/runs/ffchthvw\" target=\"_blank\">https://wandb.ai/pacman_dqn/Breakout/runs/ffchthvw</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210414_201643-ffchthvw</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f0905c12a50>"
            ],
            "text/html": [
              "<h1>Run(ffchthvw)</h1><iframe src=\"https://wandb.ai/pacman_dqn/Breakout/runs/ffchthvw\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lak3GKqz2dta",
        "outputId": "12cd0dff-66b2-4f7b-e2a9-42d98af21de3"
      },
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "buffer = []\n",
        "total_steps = 0\n",
        "total_episodes = 0\n",
        "Terminal = False\n",
        "tiebreaker = count()\n",
        "#directory = './video'\n",
        "#env = Recorder(env, directory)\n",
        "\n",
        "for ep in range(1, 20000):\n",
        "\n",
        "  state = env.reset()\n",
        "  episodic_reward = 0\n",
        "\n",
        "  for step in range(1, max_steps_per_episode):\n",
        "    \n",
        "    state = tf.expand_dims(state, 0)\n",
        "    all_actions = model(state, training=False)\n",
        "    max_action = np.argmax(all_actions[0])\n",
        "    action = epsilon_greedy(max_action, total_steps)\n",
        "\n",
        "    next_state, reward, Terminal, _ = env.step(action)\n",
        "\n",
        "    transition = (state, action, reward, next_state, Terminal)\n",
        "    td_error = calculate_td_error(transition)\n",
        "    \n",
        "    # Use square of td_error to account for negatives\n",
        "    square_error = td_error * td_error\n",
        "\n",
        "    # Sort buffer by square error and use arbitrary count() as tiebreaker\n",
        "    buffer.append((square_error, next(tiebreaker), transition))\n",
        "    heapq.heapify(buffer)\n",
        "\n",
        "    episodic_reward += reward\n",
        "    total_steps += 1\n",
        "    state = next_state\n",
        "\n",
        "    # Perform replay every 4 steps\n",
        "    if len(buffer) > batch_size and total_steps % replay_steps == 0 :\n",
        "      replay(batch_size)\n",
        "    \n",
        "    # Update target model \n",
        "    if total_steps % target_update == 0:\n",
        "      target_model.set_weights(model.get_weights())\n",
        "\n",
        "    # Keep buffer to specified length\n",
        "    if len(buffer) > max_buffer:\n",
        "      del buffer[:1]\n",
        "\n",
        "    if Terminal:\n",
        "      if wb:\n",
        "        wandb.log({'episodes:': ep, 'episodic_reward': episodic_reward, 'steps': step, 'average_steps_per_episode': total_steps/ep})\n",
        "      print(\"episode: {}/{}, reward: {}\".format(ep, 20000, episodic_reward))\n",
        "      break\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epsilon =  1.0 at step  0\n",
            "episode: 1/20000, reward: 0.0\n",
            "episode: 2/20000, reward: 0.0\n",
            "episode: 3/20000, reward: 1.0\n",
            "episode: 4/20000, reward: 0.0\n",
            "episode: 5/20000, reward: 0.0\n",
            "episode: 6/20000, reward: 0.0\n",
            "epsilon =  0.997 at step  1000\n",
            "episode: 7/20000, reward: 1.0\n",
            "episode: 8/20000, reward: 3.0\n",
            "episode: 9/20000, reward: 2.0\n",
            "episode: 10/20000, reward: 1.0\n",
            "episode: 11/20000, reward: 0.0\n",
            "episode: 12/20000, reward: 1.0\n",
            "epsilon =  0.994 at step  2000\n",
            "episode: 13/20000, reward: 1.0\n",
            "episode: 14/20000, reward: 1.0\n",
            "episode: 15/20000, reward: 0.0\n",
            "episode: 16/20000, reward: 0.0\n",
            "episode: 17/20000, reward: 0.0\n",
            "episode: 18/20000, reward: 0.0\n",
            "epsilon =  0.991 at step  3000\n",
            "episode: 19/20000, reward: 2.0\n",
            "episode: 20/20000, reward: 0.0\n",
            "episode: 21/20000, reward: 1.0\n",
            "episode: 22/20000, reward: 0.0\n",
            "episode: 23/20000, reward: 0.0\n",
            "episode: 24/20000, reward: 2.0\n",
            "epsilon =  0.988 at step  4000\n",
            "episode: 25/20000, reward: 2.0\n",
            "episode: 26/20000, reward: 1.0\n",
            "episode: 27/20000, reward: 2.0\n",
            "episode: 28/20000, reward: 1.0\n",
            "episode: 29/20000, reward: 0.0\n",
            "epsilon =  0.985 at step  5000\n",
            "episode: 30/20000, reward: 3.0\n",
            "episode: 31/20000, reward: 0.0\n",
            "episode: 32/20000, reward: 2.0\n",
            "episode: 33/20000, reward: 2.0\n",
            "episode: 34/20000, reward: 1.0\n",
            "episode: 35/20000, reward: 0.0\n",
            "epsilon =  0.982 at step  6000\n",
            "episode: 36/20000, reward: 1.0\n",
            "episode: 37/20000, reward: 0.0\n",
            "episode: 38/20000, reward: 4.0\n",
            "episode: 39/20000, reward: 0.0\n",
            "episode: 40/20000, reward: 3.0\n",
            "epsilon =  0.979 at step  7000\n",
            "episode: 41/20000, reward: 3.0\n",
            "episode: 42/20000, reward: 2.0\n",
            "episode: 43/20000, reward: 3.0\n",
            "episode: 44/20000, reward: 1.0\n",
            "epsilon =  0.976 at step  8000\n",
            "episode: 45/20000, reward: 3.0\n",
            "episode: 46/20000, reward: 0.0\n",
            "episode: 47/20000, reward: 5.0\n",
            "episode: 48/20000, reward: 1.0\n",
            "episode: 49/20000, reward: 1.0\n",
            "epsilon =  0.973 at step  9000\n",
            "episode: 50/20000, reward: 2.0\n",
            "episode: 51/20000, reward: 1.0\n",
            "episode: 52/20000, reward: 0.0\n",
            "episode: 53/20000, reward: 1.0\n",
            "episode: 54/20000, reward: 1.0\n",
            "episode: 55/20000, reward: 1.0\n",
            "epsilon =  0.97 at step  10000\n",
            "episode: 56/20000, reward: 0.0\n",
            "episode: 57/20000, reward: 2.0\n",
            "episode: 58/20000, reward: 1.0\n",
            "episode: 59/20000, reward: 3.0\n",
            "episode: 60/20000, reward: 1.0\n",
            "epsilon =  0.967 at step  11000\n",
            "episode: 61/20000, reward: 0.0\n",
            "episode: 62/20000, reward: 0.0\n",
            "episode: 63/20000, reward: 1.0\n",
            "episode: 64/20000, reward: 3.0\n",
            "episode: 65/20000, reward: 1.0\n",
            "episode: 66/20000, reward: 1.0\n",
            "epsilon =  0.964 at step  12000\n",
            "episode: 67/20000, reward: 4.0\n",
            "episode: 68/20000, reward: 0.0\n",
            "episode: 69/20000, reward: 1.0\n",
            "episode: 70/20000, reward: 0.0\n",
            "episode: 71/20000, reward: 0.0\n",
            "episode: 72/20000, reward: 0.0\n",
            "epsilon =  0.961 at step  13000\n",
            "episode: 73/20000, reward: 0.0\n",
            "episode: 74/20000, reward: 3.0\n",
            "episode: 75/20000, reward: 2.0\n",
            "episode: 76/20000, reward: 1.0\n",
            "episode: 77/20000, reward: 1.0\n",
            "epsilon =  0.958 at step  14000\n",
            "episode: 78/20000, reward: 2.0\n",
            "episode: 79/20000, reward: 1.0\n",
            "episode: 80/20000, reward: 2.0\n",
            "episode: 81/20000, reward: 0.0\n",
            "episode: 82/20000, reward: 0.0\n",
            "episode: 83/20000, reward: 1.0\n",
            "epsilon =  0.955 at step  15000\n",
            "episode: 84/20000, reward: 1.0\n",
            "episode: 85/20000, reward: 0.0\n",
            "episode: 86/20000, reward: 1.0\n",
            "episode: 87/20000, reward: 0.0\n",
            "episode: 88/20000, reward: 4.0\n",
            "episode: 89/20000, reward: 0.0\n",
            "epsilon =  0.952 at step  16000\n",
            "episode: 90/20000, reward: 0.0\n",
            "episode: 91/20000, reward: 2.0\n",
            "episode: 92/20000, reward: 0.0\n",
            "episode: 93/20000, reward: 1.0\n",
            "episode: 94/20000, reward: 1.0\n",
            "episode: 95/20000, reward: 0.0\n",
            "epsilon =  0.949 at step  17000\n",
            "episode: 96/20000, reward: 2.0\n",
            "episode: 97/20000, reward: 0.0\n",
            "episode: 98/20000, reward: 3.0\n",
            "episode: 99/20000, reward: 2.0\n",
            "epsilon =  0.946 at step  18000\n",
            "episode: 100/20000, reward: 4.0\n",
            "episode: 101/20000, reward: 1.0\n",
            "episode: 102/20000, reward: 1.0\n",
            "episode: 103/20000, reward: 2.0\n",
            "episode: 104/20000, reward: 3.0\n",
            "epsilon =  0.943 at step  19000\n",
            "episode: 105/20000, reward: 1.0\n",
            "episode: 106/20000, reward: 3.0\n",
            "episode: 107/20000, reward: 0.0\n",
            "episode: 108/20000, reward: 0.0\n",
            "episode: 109/20000, reward: 0.0\n",
            "episode: 110/20000, reward: 0.0\n",
            "episode: 111/20000, reward: 0.0\n",
            "epsilon =  0.94 at step  20000\n",
            "episode: 112/20000, reward: 2.0\n",
            "episode: 113/20000, reward: 0.0\n",
            "episode: 114/20000, reward: 1.0\n",
            "episode: 115/20000, reward: 0.0\n",
            "episode: 116/20000, reward: 0.0\n",
            "episode: 117/20000, reward: 0.0\n",
            "epsilon =  0.937 at step  21000\n",
            "episode: 118/20000, reward: 0.0\n",
            "episode: 119/20000, reward: 0.0\n",
            "episode: 120/20000, reward: 3.0\n",
            "episode: 121/20000, reward: 0.0\n",
            "episode: 122/20000, reward: 3.0\n",
            "epsilon =  0.9339999999999999 at step  22000\n",
            "episode: 123/20000, reward: 5.0\n",
            "episode: 124/20000, reward: 0.0\n",
            "episode: 125/20000, reward: 3.0\n",
            "episode: 126/20000, reward: 2.0\n",
            "epsilon =  0.931 at step  23000\n",
            "episode: 127/20000, reward: 3.0\n",
            "episode: 128/20000, reward: 0.0\n",
            "episode: 129/20000, reward: 3.0\n",
            "episode: 130/20000, reward: 1.0\n",
            "episode: 131/20000, reward: 1.0\n",
            "epsilon =  0.928 at step  24000\n",
            "episode: 132/20000, reward: 3.0\n",
            "episode: 133/20000, reward: 1.0\n",
            "episode: 134/20000, reward: 3.0\n",
            "episode: 135/20000, reward: 0.0\n",
            "episode: 136/20000, reward: 2.0\n",
            "epsilon =  0.925 at step  25000\n",
            "episode: 137/20000, reward: 3.0\n",
            "episode: 138/20000, reward: 0.0\n",
            "episode: 139/20000, reward: 5.0\n",
            "episode: 140/20000, reward: 0.0\n",
            "episode: 141/20000, reward: 1.0\n",
            "epsilon =  0.922 at step  26000\n",
            "episode: 142/20000, reward: 1.0\n",
            "episode: 143/20000, reward: 0.0\n",
            "episode: 144/20000, reward: 0.0\n",
            "episode: 145/20000, reward: 0.0\n",
            "episode: 146/20000, reward: 1.0\n",
            "episode: 147/20000, reward: 0.0\n",
            "epsilon =  0.919 at step  27000\n",
            "episode: 148/20000, reward: 2.0\n",
            "episode: 149/20000, reward: 1.0\n",
            "episode: 150/20000, reward: 2.0\n",
            "episode: 151/20000, reward: 5.0\n",
            "episode: 152/20000, reward: 0.0\n",
            "epsilon =  0.916 at step  28000\n",
            "episode: 153/20000, reward: 3.0\n",
            "episode: 154/20000, reward: 1.0\n",
            "episode: 155/20000, reward: 0.0\n",
            "episode: 156/20000, reward: 0.0\n",
            "episode: 157/20000, reward: 3.0\n",
            "episode: 158/20000, reward: 0.0\n",
            "epsilon =  0.913 at step  29000\n",
            "episode: 159/20000, reward: 1.0\n",
            "episode: 160/20000, reward: 0.0\n",
            "episode: 161/20000, reward: 2.0\n",
            "episode: 162/20000, reward: 0.0\n",
            "episode: 163/20000, reward: 0.0\n",
            "episode: 164/20000, reward: 0.0\n",
            "epsilon =  0.91 at step  30000\n",
            "episode: 165/20000, reward: 2.0\n",
            "episode: 166/20000, reward: 2.0\n",
            "episode: 167/20000, reward: 1.0\n",
            "episode: 168/20000, reward: 0.0\n",
            "episode: 169/20000, reward: 1.0\n",
            "episode: 170/20000, reward: 0.0\n",
            "epsilon =  0.907 at step  31000\n",
            "episode: 171/20000, reward: 0.0\n",
            "episode: 172/20000, reward: 3.0\n",
            "episode: 173/20000, reward: 3.0\n",
            "episode: 174/20000, reward: 1.0\n",
            "episode: 175/20000, reward: 0.0\n",
            "epsilon =  0.904 at step  32000\n",
            "episode: 176/20000, reward: 1.0\n",
            "episode: 177/20000, reward: 4.0\n",
            "episode: 178/20000, reward: 2.0\n",
            "episode: 179/20000, reward: 2.0\n",
            "episode: 180/20000, reward: 0.0\n",
            "epsilon =  0.901 at step  33000\n",
            "episode: 181/20000, reward: 3.0\n",
            "episode: 182/20000, reward: 0.0\n",
            "episode: 183/20000, reward: 0.0\n",
            "episode: 184/20000, reward: 1.0\n",
            "epsilon =  0.898 at step  34000\n",
            "episode: 185/20000, reward: 4.0\n",
            "episode: 186/20000, reward: 0.0\n",
            "episode: 187/20000, reward: 0.0\n",
            "episode: 188/20000, reward: 1.0\n",
            "episode: 189/20000, reward: 1.0\n",
            "episode: 190/20000, reward: 1.0\n",
            "episode: 191/20000, reward: 0.0\n",
            "epsilon =  0.895 at step  35000\n",
            "episode: 192/20000, reward: 4.0\n",
            "episode: 193/20000, reward: 1.0\n",
            "episode: 194/20000, reward: 2.0\n",
            "episode: 195/20000, reward: 0.0\n",
            "episode: 196/20000, reward: 1.0\n",
            "epsilon =  0.892 at step  36000\n",
            "episode: 197/20000, reward: 1.0\n",
            "episode: 198/20000, reward: 2.0\n",
            "episode: 199/20000, reward: 3.0\n",
            "episode: 200/20000, reward: 4.0\n",
            "epsilon =  0.889 at step  37000\n",
            "episode: 201/20000, reward: 1.0\n",
            "episode: 202/20000, reward: 1.0\n",
            "episode: 203/20000, reward: 0.0\n",
            "episode: 204/20000, reward: 3.0\n",
            "epsilon =  0.886 at step  38000\n",
            "episode: 205/20000, reward: 9.0\n",
            "episode: 206/20000, reward: 3.0\n",
            "episode: 207/20000, reward: 0.0\n",
            "episode: 208/20000, reward: 0.0\n",
            "episode: 209/20000, reward: 0.0\n",
            "episode: 210/20000, reward: 0.0\n",
            "epsilon =  0.883 at step  39000\n",
            "episode: 211/20000, reward: 1.0\n",
            "episode: 212/20000, reward: 0.0\n",
            "episode: 213/20000, reward: 0.0\n",
            "episode: 214/20000, reward: 1.0\n",
            "episode: 215/20000, reward: 2.0\n",
            "episode: 216/20000, reward: 1.0\n",
            "epsilon =  0.88 at step  40000\n",
            "episode: 217/20000, reward: 0.0\n",
            "episode: 218/20000, reward: 2.0\n",
            "episode: 219/20000, reward: 3.0\n",
            "episode: 220/20000, reward: 0.0\n",
            "episode: 221/20000, reward: 3.0\n",
            "epsilon =  0.877 at step  41000\n",
            "episode: 222/20000, reward: 0.0\n",
            "episode: 223/20000, reward: 3.0\n",
            "episode: 224/20000, reward: 0.0\n",
            "episode: 225/20000, reward: 0.0\n",
            "episode: 226/20000, reward: 2.0\n",
            "epsilon =  0.874 at step  42000\n",
            "episode: 227/20000, reward: 3.0\n",
            "episode: 228/20000, reward: 0.0\n",
            "episode: 229/20000, reward: 0.0\n",
            "episode: 230/20000, reward: 0.0\n",
            "episode: 231/20000, reward: 0.0\n",
            "episode: 232/20000, reward: 1.0\n",
            "episode: 233/20000, reward: 0.0\n",
            "epsilon =  0.871 at step  43000\n",
            "episode: 234/20000, reward: 0.0\n",
            "episode: 235/20000, reward: 2.0\n",
            "episode: 236/20000, reward: 0.0\n",
            "episode: 237/20000, reward: 2.0\n",
            "episode: 238/20000, reward: 2.0\n",
            "epsilon =  0.868 at step  44000\n",
            "episode: 239/20000, reward: 2.0\n",
            "episode: 240/20000, reward: 3.0\n",
            "episode: 241/20000, reward: 5.0\n",
            "episode: 242/20000, reward: 0.0\n",
            "epsilon =  0.865 at step  45000\n",
            "episode: 243/20000, reward: 2.0\n",
            "episode: 244/20000, reward: 4.0\n",
            "episode: 245/20000, reward: 4.0\n",
            "episode: 246/20000, reward: 3.0\n",
            "epsilon =  0.862 at step  46000\n",
            "episode: 247/20000, reward: 0.0\n",
            "episode: 248/20000, reward: 2.0\n",
            "episode: 249/20000, reward: 1.0\n",
            "episode: 250/20000, reward: 1.0\n",
            "episode: 251/20000, reward: 0.0\n",
            "epsilon =  0.859 at step  47000\n",
            "episode: 252/20000, reward: 6.0\n",
            "episode: 253/20000, reward: 1.0\n",
            "episode: 254/20000, reward: 2.0\n",
            "episode: 255/20000, reward: 0.0\n",
            "episode: 256/20000, reward: 0.0\n",
            "epsilon =  0.856 at step  48000\n",
            "episode: 257/20000, reward: 6.0\n",
            "episode: 258/20000, reward: 0.0\n",
            "episode: 259/20000, reward: 5.0\n",
            "epsilon =  0.853 at step  49000\n",
            "episode: 260/20000, reward: 10.0\n",
            "episode: 261/20000, reward: 0.0\n",
            "episode: 262/20000, reward: 0.0\n",
            "episode: 263/20000, reward: 0.0\n",
            "episode: 264/20000, reward: 0.0\n",
            "episode: 265/20000, reward: 2.0\n",
            "episode: 266/20000, reward: 2.0\n",
            "epsilon =  0.85 at step  50000\n",
            "episode: 267/20000, reward: 2.0\n",
            "episode: 268/20000, reward: 2.0\n",
            "episode: 269/20000, reward: 1.0\n",
            "episode: 270/20000, reward: 1.0\n",
            "episode: 271/20000, reward: 1.0\n",
            "epsilon =  0.847 at step  51000\n",
            "episode: 272/20000, reward: 1.0\n",
            "episode: 273/20000, reward: 0.0\n",
            "episode: 274/20000, reward: 2.0\n",
            "episode: 275/20000, reward: 0.0\n",
            "episode: 276/20000, reward: 0.0\n",
            "episode: 277/20000, reward: 0.0\n",
            "epsilon =  0.844 at step  52000\n",
            "episode: 278/20000, reward: 0.0\n",
            "episode: 279/20000, reward: 4.0\n",
            "episode: 280/20000, reward: 3.0\n",
            "episode: 281/20000, reward: 1.0\n",
            "episode: 282/20000, reward: 1.0\n",
            "epsilon =  0.841 at step  53000\n",
            "episode: 283/20000, reward: 0.0\n",
            "episode: 284/20000, reward: 0.0\n",
            "episode: 285/20000, reward: 2.0\n",
            "episode: 286/20000, reward: 0.0\n",
            "episode: 287/20000, reward: 2.0\n",
            "episode: 288/20000, reward: 1.0\n",
            "epsilon =  0.838 at step  54000\n",
            "episode: 289/20000, reward: 1.0\n",
            "episode: 290/20000, reward: 1.0\n",
            "episode: 291/20000, reward: 2.0\n",
            "episode: 292/20000, reward: 0.0\n",
            "episode: 293/20000, reward: 1.0\n",
            "epsilon =  0.835 at step  55000\n",
            "episode: 294/20000, reward: 2.0\n",
            "episode: 295/20000, reward: 1.0\n",
            "episode: 296/20000, reward: 0.0\n",
            "episode: 297/20000, reward: 4.0\n",
            "episode: 298/20000, reward: 2.0\n",
            "episode: 299/20000, reward: 1.0\n",
            "epsilon =  0.832 at step  56000\n",
            "episode: 300/20000, reward: 1.0\n",
            "episode: 301/20000, reward: 2.0\n",
            "episode: 302/20000, reward: 1.0\n",
            "episode: 303/20000, reward: 5.0\n",
            "epsilon =  0.829 at step  57000\n",
            "episode: 304/20000, reward: 2.0\n",
            "episode: 305/20000, reward: 2.0\n",
            "episode: 306/20000, reward: 0.0\n",
            "episode: 307/20000, reward: 0.0\n",
            "episode: 308/20000, reward: 1.0\n",
            "episode: 309/20000, reward: 0.0\n",
            "epsilon =  0.8260000000000001 at step  58000\n",
            "episode: 310/20000, reward: 0.0\n",
            "episode: 311/20000, reward: 0.0\n",
            "episode: 312/20000, reward: 2.0\n",
            "episode: 313/20000, reward: 0.0\n",
            "episode: 314/20000, reward: 1.0\n",
            "episode: 315/20000, reward: 0.0\n",
            "episode: 316/20000, reward: 1.0\n",
            "epsilon =  0.823 at step  59000\n",
            "episode: 317/20000, reward: 3.0\n",
            "episode: 318/20000, reward: 0.0\n",
            "episode: 319/20000, reward: 2.0\n",
            "episode: 320/20000, reward: 1.0\n",
            "episode: 321/20000, reward: 3.0\n",
            "epsilon =  0.8200000000000001 at step  60000\n",
            "episode: 322/20000, reward: 0.0\n",
            "episode: 323/20000, reward: 1.0\n",
            "episode: 324/20000, reward: 1.0\n",
            "episode: 325/20000, reward: 1.0\n",
            "episode: 326/20000, reward: 1.0\n",
            "episode: 327/20000, reward: 0.0\n",
            "epsilon =  0.817 at step  61000\n",
            "episode: 328/20000, reward: 3.0\n",
            "episode: 329/20000, reward: 3.0\n",
            "episode: 330/20000, reward: 1.0\n",
            "episode: 331/20000, reward: 0.0\n",
            "epsilon =  0.8140000000000001 at step  62000\n",
            "episode: 332/20000, reward: 5.0\n",
            "episode: 333/20000, reward: 0.0\n",
            "episode: 334/20000, reward: 5.0\n",
            "episode: 335/20000, reward: 0.0\n",
            "episode: 336/20000, reward: 1.0\n",
            "epsilon =  0.8109999999999999 at step  63000\n",
            "episode: 337/20000, reward: 1.0\n",
            "episode: 338/20000, reward: 2.0\n",
            "episode: 339/20000, reward: 1.0\n",
            "episode: 340/20000, reward: 0.0\n",
            "episode: 341/20000, reward: 0.0\n",
            "episode: 342/20000, reward: 0.0\n",
            "epsilon =  0.808 at step  64000\n",
            "episode: 343/20000, reward: 1.0\n",
            "episode: 344/20000, reward: 0.0\n",
            "episode: 345/20000, reward: 2.0\n",
            "episode: 346/20000, reward: 2.0\n",
            "episode: 347/20000, reward: 3.0\n",
            "epsilon =  0.8049999999999999 at step  65000\n",
            "episode: 348/20000, reward: 3.0\n",
            "episode: 349/20000, reward: 3.0\n",
            "episode: 350/20000, reward: 3.0\n",
            "episode: 351/20000, reward: 0.0\n",
            "epsilon =  0.802 at step  66000\n",
            "episode: 352/20000, reward: 0.0\n",
            "episode: 353/20000, reward: 2.0\n",
            "episode: 354/20000, reward: 1.0\n",
            "episode: 355/20000, reward: 0.0\n",
            "episode: 356/20000, reward: 0.0\n",
            "episode: 357/20000, reward: 1.0\n",
            "epsilon =  0.7989999999999999 at step  67000\n",
            "episode: 358/20000, reward: 2.0\n",
            "episode: 359/20000, reward: 2.0\n",
            "episode: 360/20000, reward: 0.0\n",
            "episode: 361/20000, reward: 1.0\n",
            "episode: 362/20000, reward: 2.0\n",
            "episode: 363/20000, reward: 0.0\n",
            "epsilon =  0.796 at step  68000\n",
            "episode: 364/20000, reward: 3.0\n",
            "episode: 365/20000, reward: 2.0\n",
            "episode: 366/20000, reward: 5.0\n",
            "epsilon =  0.793 at step  69000\n",
            "episode: 367/20000, reward: 9.0\n",
            "episode: 368/20000, reward: 0.0\n",
            "episode: 369/20000, reward: 0.0\n",
            "episode: 370/20000, reward: 0.0\n",
            "episode: 371/20000, reward: 2.0\n",
            "epsilon =  0.79 at step  70000\n",
            "episode: 372/20000, reward: 2.0\n",
            "episode: 373/20000, reward: 0.0\n",
            "episode: 374/20000, reward: 0.0\n",
            "episode: 375/20000, reward: 3.0\n",
            "episode: 376/20000, reward: 3.0\n",
            "episode: 377/20000, reward: 0.0\n",
            "epsilon =  0.787 at step  71000\n",
            "episode: 378/20000, reward: 0.0\n",
            "episode: 379/20000, reward: 0.0\n",
            "episode: 380/20000, reward: 0.0\n",
            "episode: 381/20000, reward: 0.0\n",
            "episode: 382/20000, reward: 0.0\n",
            "episode: 383/20000, reward: 0.0\n",
            "episode: 384/20000, reward: 1.0\n",
            "epsilon =  0.784 at step  72000\n",
            "episode: 385/20000, reward: 3.0\n",
            "episode: 386/20000, reward: 3.0\n",
            "episode: 387/20000, reward: 1.0\n",
            "episode: 388/20000, reward: 2.0\n",
            "epsilon =  0.781 at step  73000\n",
            "episode: 389/20000, reward: 2.0\n",
            "episode: 390/20000, reward: 2.0\n",
            "episode: 391/20000, reward: 2.0\n",
            "episode: 392/20000, reward: 0.0\n",
            "episode: 393/20000, reward: 0.0\n",
            "epsilon =  0.778 at step  74000\n",
            "episode: 394/20000, reward: 5.0\n",
            "episode: 395/20000, reward: 4.0\n",
            "episode: 396/20000, reward: 0.0\n",
            "episode: 397/20000, reward: 0.0\n",
            "episode: 398/20000, reward: 0.0\n",
            "epsilon =  0.775 at step  75000\n",
            "episode: 399/20000, reward: 7.0\n",
            "episode: 400/20000, reward: 2.0\n",
            "episode: 401/20000, reward: 3.0\n",
            "episode: 402/20000, reward: 0.0\n",
            "epsilon =  0.772 at step  76000\n",
            "episode: 403/20000, reward: 0.0\n",
            "episode: 404/20000, reward: 0.0\n",
            "episode: 405/20000, reward: 2.0\n",
            "episode: 406/20000, reward: 4.0\n",
            "episode: 407/20000, reward: 1.0\n",
            "epsilon =  0.769 at step  77000\n",
            "episode: 408/20000, reward: 1.0\n",
            "episode: 409/20000, reward: 0.0\n",
            "episode: 410/20000, reward: 1.0\n",
            "episode: 411/20000, reward: 3.0\n",
            "episode: 412/20000, reward: 2.0\n",
            "epsilon =  0.766 at step  78000\n",
            "episode: 413/20000, reward: 4.0\n",
            "episode: 414/20000, reward: 2.0\n",
            "episode: 415/20000, reward: 1.0\n",
            "episode: 416/20000, reward: 2.0\n",
            "episode: 417/20000, reward: 0.0\n",
            "epsilon =  0.763 at step  79000\n",
            "episode: 418/20000, reward: 0.0\n",
            "episode: 419/20000, reward: 1.0\n",
            "episode: 420/20000, reward: 2.0\n",
            "episode: 421/20000, reward: 0.0\n",
            "episode: 422/20000, reward: 2.0\n",
            "epsilon =  0.76 at step  80000\n",
            "episode: 423/20000, reward: 2.0\n",
            "episode: 424/20000, reward: 4.0\n",
            "episode: 425/20000, reward: 3.0\n",
            "episode: 426/20000, reward: 4.0\n",
            "epsilon =  0.757 at step  81000\n",
            "episode: 427/20000, reward: 3.0\n",
            "episode: 428/20000, reward: 0.0\n",
            "episode: 429/20000, reward: 1.0\n",
            "episode: 430/20000, reward: 1.0\n",
            "episode: 431/20000, reward: 0.0\n",
            "epsilon =  0.754 at step  82000\n",
            "episode: 432/20000, reward: 1.0\n",
            "episode: 433/20000, reward: 0.0\n",
            "episode: 434/20000, reward: 1.0\n",
            "episode: 435/20000, reward: 1.0\n",
            "episode: 436/20000, reward: 5.0\n",
            "epsilon =  0.751 at step  83000\n",
            "episode: 437/20000, reward: 0.0\n",
            "episode: 438/20000, reward: 0.0\n",
            "episode: 439/20000, reward: 0.0\n",
            "episode: 440/20000, reward: 2.0\n",
            "episode: 441/20000, reward: 0.0\n",
            "episode: 442/20000, reward: 0.0\n",
            "epsilon =  0.748 at step  84000\n",
            "episode: 443/20000, reward: 3.0\n",
            "episode: 444/20000, reward: 0.0\n",
            "episode: 445/20000, reward: 0.0\n",
            "episode: 446/20000, reward: 0.0\n",
            "episode: 447/20000, reward: 0.0\n",
            "episode: 448/20000, reward: 0.0\n",
            "episode: 449/20000, reward: 0.0\n",
            "epsilon =  0.745 at step  85000\n",
            "episode: 450/20000, reward: 1.0\n",
            "episode: 451/20000, reward: 2.0\n",
            "episode: 452/20000, reward: 0.0\n",
            "episode: 453/20000, reward: 2.0\n",
            "episode: 454/20000, reward: 0.0\n",
            "episode: 455/20000, reward: 1.0\n",
            "epsilon =  0.742 at step  86000\n",
            "episode: 456/20000, reward: 0.0\n",
            "episode: 457/20000, reward: 1.0\n",
            "episode: 458/20000, reward: 0.0\n",
            "episode: 459/20000, reward: 4.0\n",
            "epsilon =  0.739 at step  87000\n",
            "episode: 460/20000, reward: 6.0\n",
            "episode: 461/20000, reward: 0.0\n",
            "episode: 462/20000, reward: 2.0\n",
            "episode: 463/20000, reward: 0.0\n",
            "episode: 464/20000, reward: 1.0\n",
            "episode: 465/20000, reward: 2.0\n",
            "epsilon =  0.736 at step  88000\n",
            "episode: 466/20000, reward: 2.0\n",
            "episode: 467/20000, reward: 2.0\n",
            "episode: 468/20000, reward: 4.0\n",
            "episode: 469/20000, reward: 1.0\n",
            "epsilon =  0.733 at step  89000\n",
            "episode: 470/20000, reward: 2.0\n",
            "episode: 471/20000, reward: 0.0\n",
            "episode: 472/20000, reward: 1.0\n",
            "episode: 473/20000, reward: 3.0\n",
            "episode: 474/20000, reward: 2.0\n",
            "epsilon =  0.73 at step  90000\n",
            "episode: 475/20000, reward: 0.0\n",
            "episode: 476/20000, reward: 3.0\n",
            "episode: 477/20000, reward: 0.0\n",
            "episode: 478/20000, reward: 0.0\n",
            "episode: 479/20000, reward: 2.0\n",
            "epsilon =  0.727 at step  91000\n",
            "episode: 480/20000, reward: 2.0\n",
            "episode: 481/20000, reward: 0.0\n",
            "episode: 482/20000, reward: 1.0\n",
            "episode: 483/20000, reward: 3.0\n",
            "episode: 484/20000, reward: 4.0\n",
            "epsilon =  0.724 at step  92000\n",
            "episode: 485/20000, reward: 2.0\n",
            "episode: 486/20000, reward: 1.0\n",
            "episode: 487/20000, reward: 0.0\n",
            "episode: 488/20000, reward: 2.0\n",
            "epsilon =  0.721 at step  93000\n",
            "episode: 489/20000, reward: 5.0\n",
            "episode: 490/20000, reward: 0.0\n",
            "episode: 491/20000, reward: 1.0\n",
            "episode: 492/20000, reward: 2.0\n",
            "episode: 493/20000, reward: 2.0\n",
            "epsilon =  0.718 at step  94000\n",
            "episode: 494/20000, reward: 2.0\n",
            "episode: 495/20000, reward: 0.0\n",
            "episode: 496/20000, reward: 0.0\n",
            "episode: 497/20000, reward: 2.0\n",
            "episode: 498/20000, reward: 3.0\n",
            "episode: 499/20000, reward: 0.0\n",
            "epsilon =  0.7150000000000001 at step  95000\n",
            "episode: 500/20000, reward: 2.0\n",
            "episode: 501/20000, reward: 3.0\n",
            "episode: 502/20000, reward: 0.0\n",
            "episode: 503/20000, reward: 0.0\n",
            "episode: 504/20000, reward: 0.0\n",
            "epsilon =  0.712 at step  96000\n",
            "episode: 505/20000, reward: 3.0\n",
            "episode: 506/20000, reward: 1.0\n",
            "episode: 507/20000, reward: 0.0\n",
            "episode: 508/20000, reward: 0.0\n",
            "episode: 509/20000, reward: 1.0\n",
            "epsilon =  0.7090000000000001 at step  97000\n",
            "episode: 510/20000, reward: 4.0\n",
            "episode: 511/20000, reward: 1.0\n",
            "episode: 512/20000, reward: 1.0\n",
            "episode: 513/20000, reward: 0.0\n",
            "episode: 514/20000, reward: 0.0\n",
            "episode: 515/20000, reward: 3.0\n",
            "epsilon =  0.706 at step  98000\n",
            "episode: 516/20000, reward: 3.0\n",
            "episode: 517/20000, reward: 0.0\n",
            "episode: 518/20000, reward: 0.0\n",
            "episode: 519/20000, reward: 0.0\n",
            "episode: 520/20000, reward: 0.0\n",
            "epsilon =  0.7030000000000001 at step  99000\n",
            "episode: 521/20000, reward: 4.0\n",
            "episode: 522/20000, reward: 1.0\n",
            "episode: 523/20000, reward: 4.0\n",
            "episode: 524/20000, reward: 0.0\n",
            "episode: 525/20000, reward: 1.0\n",
            "epsilon =  0.7 at step  100000\n",
            "episode: 526/20000, reward: 3.0\n",
            "episode: 527/20000, reward: 0.0\n",
            "episode: 528/20000, reward: 2.0\n",
            "episode: 529/20000, reward: 2.0\n",
            "episode: 530/20000, reward: 2.0\n",
            "epsilon =  0.6970000000000001 at step  101000\n",
            "episode: 531/20000, reward: 3.0\n",
            "episode: 532/20000, reward: 2.0\n",
            "episode: 533/20000, reward: 2.0\n",
            "episode: 534/20000, reward: 0.0\n",
            "episode: 535/20000, reward: 0.0\n",
            "episode: 536/20000, reward: 0.0\n",
            "epsilon =  0.694 at step  102000\n",
            "episode: 537/20000, reward: 1.0\n",
            "episode: 538/20000, reward: 2.0\n",
            "episode: 539/20000, reward: 1.0\n",
            "episode: 540/20000, reward: 0.0\n",
            "episode: 541/20000, reward: 1.0\n",
            "epsilon =  0.6910000000000001 at step  103000\n",
            "episode: 542/20000, reward: 2.0\n",
            "episode: 543/20000, reward: 1.0\n",
            "episode: 544/20000, reward: 0.0\n",
            "episode: 545/20000, reward: 3.0\n",
            "episode: 546/20000, reward: 1.0\n",
            "epsilon =  0.688 at step  104000\n",
            "episode: 547/20000, reward: 4.0\n",
            "episode: 548/20000, reward: 4.0\n",
            "episode: 549/20000, reward: 4.0\n",
            "episode: 550/20000, reward: 0.0\n",
            "episode: 551/20000, reward: 1.0\n",
            "epsilon =  0.685 at step  105000\n",
            "episode: 552/20000, reward: 0.0\n",
            "episode: 553/20000, reward: 3.0\n",
            "episode: 554/20000, reward: 2.0\n",
            "episode: 555/20000, reward: 0.0\n",
            "epsilon =  0.6819999999999999 at step  106000\n",
            "episode: 556/20000, reward: 4.0\n",
            "episode: 557/20000, reward: 0.0\n",
            "episode: 558/20000, reward: 0.0\n",
            "episode: 559/20000, reward: 1.0\n",
            "episode: 560/20000, reward: 0.0\n",
            "episode: 561/20000, reward: 0.0\n",
            "episode: 562/20000, reward: 2.0\n",
            "epsilon =  0.679 at step  107000\n",
            "episode: 563/20000, reward: 0.0\n",
            "episode: 564/20000, reward: 0.0\n",
            "episode: 565/20000, reward: 1.0\n",
            "episode: 566/20000, reward: 1.0\n",
            "episode: 567/20000, reward: 2.0\n",
            "episode: 568/20000, reward: 0.0\n",
            "episode: 569/20000, reward: 0.0\n",
            "epsilon =  0.6759999999999999 at step  108000\n",
            "episode: 570/20000, reward: 5.0\n",
            "episode: 571/20000, reward: 0.0\n",
            "episode: 572/20000, reward: 0.0\n",
            "episode: 573/20000, reward: 2.0\n",
            "episode: 574/20000, reward: 1.0\n",
            "epsilon =  0.673 at step  109000\n",
            "episode: 575/20000, reward: 1.0\n",
            "episode: 576/20000, reward: 0.0\n",
            "episode: 577/20000, reward: 0.0\n",
            "episode: 578/20000, reward: 1.0\n",
            "episode: 579/20000, reward: 4.0\n",
            "episode: 580/20000, reward: 1.0\n",
            "epsilon =  0.6699999999999999 at step  110000\n",
            "episode: 581/20000, reward: 0.0\n",
            "episode: 582/20000, reward: 6.0\n",
            "episode: 583/20000, reward: 1.0\n",
            "episode: 584/20000, reward: 0.0\n",
            "episode: 585/20000, reward: 0.0\n",
            "epsilon =  0.667 at step  111000\n",
            "episode: 586/20000, reward: 1.0\n",
            "episode: 587/20000, reward: 3.0\n",
            "episode: 588/20000, reward: 2.0\n",
            "episode: 589/20000, reward: 1.0\n",
            "episode: 590/20000, reward: 1.0\n",
            "epsilon =  0.6639999999999999 at step  112000\n",
            "episode: 591/20000, reward: 0.0\n",
            "episode: 592/20000, reward: 0.0\n",
            "episode: 593/20000, reward: 1.0\n",
            "episode: 594/20000, reward: 2.0\n",
            "episode: 595/20000, reward: 5.0\n",
            "epsilon =  0.661 at step  113000\n",
            "episode: 596/20000, reward: 7.0\n",
            "episode: 597/20000, reward: 1.0\n",
            "episode: 598/20000, reward: 0.0\n",
            "episode: 599/20000, reward: 1.0\n",
            "episode: 600/20000, reward: 1.0\n",
            "epsilon =  0.6579999999999999 at step  114000\n",
            "episode: 601/20000, reward: 2.0\n",
            "episode: 602/20000, reward: 2.0\n",
            "episode: 603/20000, reward: 0.0\n",
            "episode: 604/20000, reward: 3.0\n",
            "episode: 605/20000, reward: 0.0\n",
            "epsilon =  0.655 at step  115000\n",
            "episode: 606/20000, reward: 1.0\n",
            "episode: 607/20000, reward: 0.0\n",
            "episode: 608/20000, reward: 0.0\n",
            "episode: 609/20000, reward: 2.0\n",
            "episode: 610/20000, reward: 0.0\n",
            "episode: 611/20000, reward: 2.0\n",
            "episode: 612/20000, reward: 0.0\n",
            "epsilon =  0.652 at step  116000\n",
            "episode: 613/20000, reward: 4.0\n",
            "episode: 614/20000, reward: 0.0\n",
            "episode: 615/20000, reward: 2.0\n",
            "episode: 616/20000, reward: 0.0\n",
            "epsilon =  0.649 at step  117000\n",
            "episode: 617/20000, reward: 2.0\n",
            "episode: 618/20000, reward: 0.0\n",
            "episode: 619/20000, reward: 2.0\n",
            "episode: 620/20000, reward: 2.0\n",
            "episode: 621/20000, reward: 2.0\n",
            "episode: 622/20000, reward: 0.0\n",
            "epsilon =  0.646 at step  118000\n",
            "episode: 623/20000, reward: 1.0\n",
            "episode: 624/20000, reward: 0.0\n",
            "episode: 625/20000, reward: 4.0\n",
            "episode: 626/20000, reward: 0.0\n",
            "episode: 628/20000, reward: 0.0\n",
            "epsilon =  0.643 at step  119000\n",
            "episode: 629/20000, reward: 4.0\n",
            "episode: 630/20000, reward: 1.0\n",
            "episode: 631/20000, reward: 3.0\n",
            "episode: 632/20000, reward: 0.0\n",
            "epsilon =  0.64 at step  120000\n",
            "episode: 633/20000, reward: 3.0\n",
            "episode: 634/20000, reward: 1.0\n",
            "episode: 635/20000, reward: 0.0\n",
            "episode: 636/20000, reward: 2.0\n",
            "episode: 637/20000, reward: 2.0\n",
            "epsilon =  0.637 at step  121000\n",
            "episode: 638/20000, reward: 0.0\n",
            "episode: 639/20000, reward: 6.0\n",
            "episode: 640/20000, reward: 0.0\n",
            "episode: 641/20000, reward: 2.0\n",
            "episode: 642/20000, reward: 0.0\n",
            "epsilon =  0.634 at step  122000\n",
            "episode: 643/20000, reward: 0.0\n",
            "episode: 644/20000, reward: 0.0\n",
            "episode: 645/20000, reward: 0.0\n",
            "episode: 646/20000, reward: 1.0\n",
            "episode: 647/20000, reward: 0.0\n",
            "episode: 648/20000, reward: 0.0\n",
            "epsilon =  0.631 at step  123000\n",
            "episode: 649/20000, reward: 0.0\n",
            "episode: 650/20000, reward: 0.0\n",
            "episode: 651/20000, reward: 1.0\n",
            "episode: 652/20000, reward: 1.0\n",
            "episode: 653/20000, reward: 0.0\n",
            "episode: 654/20000, reward: 1.0\n",
            "epsilon =  0.628 at step  124000\n",
            "episode: 655/20000, reward: 1.0\n",
            "episode: 656/20000, reward: 0.0\n",
            "episode: 657/20000, reward: 1.0\n",
            "episode: 658/20000, reward: 1.0\n",
            "episode: 659/20000, reward: 1.0\n",
            "epsilon =  0.625 at step  125000\n",
            "episode: 660/20000, reward: 2.0\n",
            "episode: 661/20000, reward: 1.0\n",
            "episode: 662/20000, reward: 2.0\n",
            "episode: 663/20000, reward: 0.0\n",
            "episode: 664/20000, reward: 1.0\n",
            "epsilon =  0.622 at step  126000\n",
            "episode: 665/20000, reward: 2.0\n",
            "episode: 666/20000, reward: 0.0\n",
            "episode: 667/20000, reward: 0.0\n",
            "episode: 668/20000, reward: 1.0\n",
            "episode: 669/20000, reward: 0.0\n",
            "episode: 670/20000, reward: 1.0\n",
            "epsilon =  0.619 at step  127000\n",
            "episode: 671/20000, reward: 1.0\n",
            "episode: 672/20000, reward: 1.0\n",
            "episode: 673/20000, reward: 1.0\n",
            "episode: 674/20000, reward: 1.0\n",
            "episode: 675/20000, reward: 1.0\n",
            "episode: 676/20000, reward: 0.0\n",
            "epsilon =  0.616 at step  128000\n",
            "episode: 677/20000, reward: 1.0\n",
            "episode: 678/20000, reward: 1.0\n",
            "episode: 679/20000, reward: 1.0\n",
            "episode: 680/20000, reward: 4.0\n",
            "epsilon =  0.613 at step  129000\n",
            "episode: 681/20000, reward: 4.0\n",
            "episode: 682/20000, reward: 2.0\n",
            "episode: 683/20000, reward: 3.0\n",
            "episode: 684/20000, reward: 1.0\n",
            "epsilon =  0.61 at step  130000\n",
            "episode: 685/20000, reward: 1.0\n",
            "episode: 686/20000, reward: 0.0\n",
            "episode: 687/20000, reward: 1.0\n",
            "episode: 688/20000, reward: 0.0\n",
            "episode: 689/20000, reward: 1.0\n",
            "epsilon =  0.607 at step  131000\n",
            "episode: 690/20000, reward: 3.0\n",
            "episode: 691/20000, reward: 1.0\n",
            "episode: 692/20000, reward: 8.0\n",
            "episode: 693/20000, reward: 0.0\n",
            "epsilon =  0.604 at step  132000\n",
            "episode: 694/20000, reward: 4.0\n",
            "episode: 695/20000, reward: 4.0\n",
            "episode: 696/20000, reward: 1.0\n",
            "episode: 697/20000, reward: 2.0\n",
            "episode: 698/20000, reward: 1.0\n",
            "epsilon =  0.601 at step  133000\n",
            "episode: 699/20000, reward: 0.0\n",
            "episode: 700/20000, reward: 0.0\n",
            "episode: 701/20000, reward: 0.0\n",
            "episode: 702/20000, reward: 0.0\n",
            "episode: 703/20000, reward: 0.0\n",
            "epsilon =  0.598 at step  134000\n",
            "episode: 704/20000, reward: 4.0\n",
            "episode: 705/20000, reward: 2.0\n",
            "episode: 706/20000, reward: 1.0\n",
            "episode: 707/20000, reward: 1.0\n",
            "episode: 708/20000, reward: 3.0\n",
            "epsilon =  0.595 at step  135000\n",
            "episode: 709/20000, reward: 1.0\n",
            "episode: 710/20000, reward: 3.0\n",
            "episode: 711/20000, reward: 0.0\n",
            "episode: 712/20000, reward: 0.0\n",
            "episode: 713/20000, reward: 0.0\n",
            "epsilon =  0.5920000000000001 at step  136000\n",
            "episode: 714/20000, reward: 0.0\n",
            "episode: 715/20000, reward: 0.0\n",
            "episode: 716/20000, reward: 0.0\n",
            "episode: 717/20000, reward: 1.0\n",
            "episode: 718/20000, reward: 1.0\n",
            "episode: 719/20000, reward: 2.0\n",
            "epsilon =  0.589 at step  137000\n",
            "episode: 720/20000, reward: 1.0\n",
            "episode: 721/20000, reward: 2.0\n",
            "episode: 722/20000, reward: 1.0\n",
            "episode: 723/20000, reward: 1.0\n",
            "epsilon =  0.5860000000000001 at step  138000\n",
            "episode: 724/20000, reward: 2.0\n",
            "episode: 725/20000, reward: 1.0\n",
            "episode: 726/20000, reward: 2.0\n",
            "episode: 727/20000, reward: 0.0\n",
            "episode: 728/20000, reward: 1.0\n",
            "epsilon =  0.583 at step  139000\n",
            "episode: 729/20000, reward: 1.0\n",
            "episode: 730/20000, reward: 0.0\n",
            "episode: 731/20000, reward: 3.0\n",
            "episode: 732/20000, reward: 2.0\n",
            "episode: 733/20000, reward: 1.0\n",
            "epsilon =  0.5800000000000001 at step  140000\n",
            "episode: 734/20000, reward: 2.0\n",
            "episode: 735/20000, reward: 0.0\n",
            "episode: 736/20000, reward: 2.0\n",
            "episode: 737/20000, reward: 0.0\n",
            "episode: 738/20000, reward: 1.0\n",
            "episode: 739/20000, reward: 0.0\n",
            "epsilon =  0.577 at step  141000\n",
            "episode: 740/20000, reward: 2.0\n",
            "episode: 741/20000, reward: 2.0\n",
            "episode: 742/20000, reward: 1.0\n",
            "episode: 743/20000, reward: 0.0\n",
            "episode: 744/20000, reward: 0.0\n",
            "epsilon =  0.5740000000000001 at step  142000\n",
            "episode: 745/20000, reward: 3.0\n",
            "episode: 746/20000, reward: 0.0\n",
            "episode: 747/20000, reward: 1.0\n",
            "episode: 748/20000, reward: 0.0\n",
            "episode: 749/20000, reward: 0.0\n",
            "epsilon =  0.571 at step  143000\n",
            "episode: 750/20000, reward: 6.0\n",
            "episode: 751/20000, reward: 1.0\n",
            "episode: 752/20000, reward: 0.0\n",
            "episode: 753/20000, reward: 3.0\n",
            "epsilon =  0.5680000000000001 at step  144000\n",
            "episode: 754/20000, reward: 0.0\n",
            "episode: 755/20000, reward: 0.0\n",
            "episode: 756/20000, reward: 1.0\n",
            "episode: 757/20000, reward: 0.0\n",
            "episode: 758/20000, reward: 1.0\n",
            "episode: 759/20000, reward: 2.0\n",
            "epsilon =  0.565 at step  145000\n",
            "episode: 760/20000, reward: 2.0\n",
            "episode: 761/20000, reward: 3.0\n",
            "episode: 762/20000, reward: 1.0\n",
            "episode: 763/20000, reward: 3.0\n",
            "episode: 764/20000, reward: 1.0\n",
            "epsilon =  0.562 at step  146000\n",
            "episode: 765/20000, reward: 2.0\n",
            "episode: 766/20000, reward: 2.0\n",
            "episode: 767/20000, reward: 3.0\n",
            "episode: 768/20000, reward: 0.0\n",
            "episode: 769/20000, reward: 0.0\n",
            "epsilon =  0.5589999999999999 at step  147000\n",
            "episode: 770/20000, reward: 0.0\n",
            "episode: 771/20000, reward: 1.0\n",
            "episode: 772/20000, reward: 1.0\n",
            "episode: 773/20000, reward: 9.0\n",
            "epsilon =  0.556 at step  148000\n",
            "episode: 774/20000, reward: 1.0\n",
            "episode: 775/20000, reward: 0.0\n",
            "episode: 776/20000, reward: 2.0\n",
            "episode: 777/20000, reward: 6.0\n",
            "epsilon =  0.5529999999999999 at step  149000\n",
            "episode: 778/20000, reward: 0.0\n",
            "episode: 779/20000, reward: 1.0\n",
            "episode: 780/20000, reward: 2.0\n",
            "episode: 781/20000, reward: 1.0\n",
            "episode: 782/20000, reward: 1.0\n",
            "epsilon =  0.55 at step  150000\n",
            "episode: 783/20000, reward: 1.0\n",
            "episode: 784/20000, reward: 1.0\n",
            "episode: 785/20000, reward: 3.0\n",
            "episode: 786/20000, reward: 3.0\n",
            "epsilon =  0.5469999999999999 at step  151000\n",
            "episode: 787/20000, reward: 1.0\n",
            "episode: 788/20000, reward: 0.0\n",
            "episode: 789/20000, reward: 1.0\n",
            "episode: 790/20000, reward: 0.0\n",
            "episode: 791/20000, reward: 3.0\n",
            "epsilon =  0.544 at step  152000\n",
            "episode: 792/20000, reward: 2.0\n",
            "episode: 793/20000, reward: 2.0\n",
            "episode: 794/20000, reward: 3.0\n",
            "episode: 795/20000, reward: 0.0\n",
            "episode: 796/20000, reward: 1.0\n",
            "episode: 797/20000, reward: 0.0\n",
            "epsilon =  0.5409999999999999 at step  153000\n",
            "episode: 798/20000, reward: 0.0\n",
            "episode: 799/20000, reward: 3.0\n",
            "episode: 800/20000, reward: 0.0\n",
            "episode: 801/20000, reward: 1.0\n",
            "episode: 802/20000, reward: 1.0\n",
            "epsilon =  0.538 at step  154000\n",
            "episode: 803/20000, reward: 1.0\n",
            "episode: 804/20000, reward: 1.0\n",
            "episode: 805/20000, reward: 1.0\n",
            "episode: 806/20000, reward: 2.0\n",
            "episode: 807/20000, reward: 1.0\n",
            "epsilon =  0.5349999999999999 at step  155000\n",
            "episode: 808/20000, reward: 0.0\n",
            "episode: 809/20000, reward: 0.0\n",
            "episode: 810/20000, reward: 1.0\n",
            "episode: 811/20000, reward: 1.0\n",
            "episode: 812/20000, reward: 0.0\n",
            "episode: 813/20000, reward: 0.0\n",
            "epsilon =  0.532 at step  156000\n",
            "episode: 814/20000, reward: 1.0\n",
            "episode: 815/20000, reward: 1.0\n",
            "episode: 816/20000, reward: 1.0\n",
            "episode: 817/20000, reward: 0.0\n",
            "episode: 818/20000, reward: 0.0\n",
            "episode: 819/20000, reward: 1.0\n",
            "epsilon =  0.529 at step  157000\n",
            "episode: 820/20000, reward: 1.0\n",
            "episode: 821/20000, reward: 0.0\n",
            "episode: 822/20000, reward: 0.0\n",
            "episode: 823/20000, reward: 3.0\n",
            "episode: 824/20000, reward: 3.0\n",
            "epsilon =  0.526 at step  158000\n",
            "episode: 825/20000, reward: 1.0\n",
            "episode: 826/20000, reward: 1.0\n",
            "episode: 827/20000, reward: 2.0\n",
            "episode: 828/20000, reward: 1.0\n",
            "episode: 829/20000, reward: 1.0\n",
            "epsilon =  0.523 at step  159000\n",
            "episode: 830/20000, reward: 2.0\n",
            "episode: 831/20000, reward: 2.0\n",
            "episode: 832/20000, reward: 2.0\n",
            "episode: 833/20000, reward: 0.0\n",
            "episode: 834/20000, reward: 0.0\n",
            "episode: 835/20000, reward: 2.0\n",
            "epsilon =  0.52 at step  160000\n",
            "episode: 836/20000, reward: 2.0\n",
            "episode: 837/20000, reward: 0.0\n",
            "episode: 838/20000, reward: 3.0\n",
            "episode: 839/20000, reward: 0.0\n",
            "epsilon =  0.517 at step  161000\n",
            "episode: 840/20000, reward: 4.0\n",
            "episode: 841/20000, reward: 1.0\n",
            "episode: 842/20000, reward: 0.0\n",
            "episode: 843/20000, reward: 2.0\n",
            "episode: 844/20000, reward: 1.0\n",
            "episode: 845/20000, reward: 0.0\n",
            "epsilon =  0.514 at step  162000\n",
            "episode: 846/20000, reward: 1.0\n",
            "episode: 847/20000, reward: 0.0\n",
            "episode: 848/20000, reward: 0.0\n",
            "episode: 849/20000, reward: 1.0\n",
            "episode: 850/20000, reward: 0.0\n",
            "episode: 851/20000, reward: 0.0\n",
            "epsilon =  0.511 at step  163000\n",
            "episode: 852/20000, reward: 2.0\n",
            "episode: 853/20000, reward: 1.0\n",
            "episode: 854/20000, reward: 3.0\n",
            "episode: 855/20000, reward: 0.0\n",
            "epsilon =  0.508 at step  164000\n",
            "episode: 856/20000, reward: 2.0\n",
            "episode: 857/20000, reward: 1.0\n",
            "episode: 858/20000, reward: 4.0\n",
            "episode: 859/20000, reward: 2.0\n",
            "epsilon =  0.505 at step  165000\n",
            "episode: 860/20000, reward: 3.0\n",
            "episode: 861/20000, reward: 0.0\n",
            "episode: 862/20000, reward: 0.0\n",
            "episode: 863/20000, reward: 0.0\n",
            "episode: 864/20000, reward: 2.0\n",
            "episode: 865/20000, reward: 0.0\n",
            "epsilon =  0.502 at step  166000\n",
            "episode: 866/20000, reward: 1.0\n",
            "episode: 867/20000, reward: 3.0\n",
            "episode: 868/20000, reward: 0.0\n",
            "episode: 869/20000, reward: 2.0\n",
            "episode: 870/20000, reward: 3.0\n",
            "epsilon =  0.499 at step  167000\n",
            "episode: 871/20000, reward: 0.0\n",
            "episode: 872/20000, reward: 0.0\n",
            "episode: 873/20000, reward: 4.0\n",
            "episode: 874/20000, reward: 0.0\n",
            "episode: 875/20000, reward: 0.0\n",
            "epsilon =  0.496 at step  168000\n",
            "episode: 876/20000, reward: 0.0\n",
            "episode: 877/20000, reward: 2.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}